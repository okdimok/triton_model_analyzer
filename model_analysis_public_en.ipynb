{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPDX-FileCopyrightText: Copyright (c) 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "\n",
    "SPDX-License-Identifier: MIT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Triton Model Analyzer\n",
    "\n",
    "## Assumptions\n",
    "You have an access to host, where you can run docker containers. The host is connected to the Internet."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your model repository\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next commands to work, it's important to clone this repository to the filesystem, which supports symbollic links, transparently for docker. Any linux disk partition is sufficient. NFS and NTFS are not."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we're using [Hi-Fi GAN](https://ngc.nvidia.com/catalog/models/nvidia:nemo:tts_hifigan) model from [NeMo](https://github.com/NVIDIA/NeMo).\n",
    "To get it's ONNX one should run the NeMo container in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker run --rm --gpus '\"device=0\"' -it --ipc=host \\\n",
    "-v $HOME/:/ext_home \\\n",
    "-v ${PWD}:${PWD} \\\n",
    "-w ${PWD} \\\n",
    "--name ${USER}_nemo \\\n",
    "nvcr.io/nvidia/nemo:1.3.0 \\\n",
    "-- python get_hifigan.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The command we run inside the container\n",
    "```\n",
    "python get_hifigan.py\n",
    "```\n",
    "is equivalent to the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.models import HifiGanModel\n",
    "\n",
    "model = HifiGanModel.from_pretrained(model_name=\"tts_hifigan\")\n",
    "model.export(\"./hifigan.onnx\")\n",
    "\n",
    "model = HifiGanModel.from_pretrained(model_name=\"tts_hifigan\")\n",
    "model.export(\"./hifigan.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the cell above, two files are to appear in the current directory: `hifigan.onnx` and `hifigan.pt`\n",
    "\n",
    "We'll need the ONNX model to experiment with Model Analyzer â€” this is the tool, that helps select the optimal inference config, within the specific backrnd. We need to copy  `hifigan.onnx` to `model_repository/hifigan/1/model.onnx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "mkdir -p model_repository/hifigan/1\n",
    "cp hifigan.onnx model_repository/hifigan/1/model.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "TorchScript will be required later, for the Model Navigator experiments. It help in selecting the most optimal backend fot the specific model.\n",
    "Having these files, the NeMo container can be stopped. For this, it's sufficient to exit its shell due to the `--rm` flag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curl and Perf Analyzer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick Triton test launch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker run --rm --gpus '\"device=0\"' -it --ipc=host \\\n",
    "-v $HOME/:/ext_home \\\n",
    "-v ${PWD}:${PWD} \\\n",
    "-w ${PWD} \\\n",
    "-p 8000:8000 \\\n",
    "-p 8001:8001 \\\n",
    "-p 8002:8002 \\\n",
    "--name ${USER}_triton \\\n",
    "nvcr.io/nvidia/tritonserver:22.12-py3 \\\n",
    "tritonserver --model-repository ${PWD}/model_repository --log-verbose 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Note, that such amount of logs can negatively affect the performance, and is recommended for debug only.\n",
    "\n",
    "After this, one can run in another terminal to quickly check, if the server works. One should expect long json as an output of the command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "curl -kv -X POST 'http://127.0.0.1:8000/v2/models/hifigan/infer' \\\n",
    " -H 'accept: application/json' \\\n",
    " -H 'Content-Type: application/octet-stream' \\\n",
    " -H 'connection: keep-alive' \\\n",
    " -d @hifigan_curl_data.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note, that it uses an HTTP protocol, which has quite high overhead.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now one can launch Triton SDK container in yet another terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker run --rm --gpus '\"device=0\"' -it --ipc=host \\\n",
    "-v $HOME/:/ext_home \\\n",
    "-v ${PWD}:${PWD} \\\n",
    "-w ${PWD} \\\n",
    "--net=host \\\n",
    "--name ${USER}_triton_sdk \\\n",
    "nvcr.io/nvidia/tritonserver:22.12-py3-sdk \\\n",
    "/bin/bash"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can save perf_analyzer help for later usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "perf_analyzer --help 2>&1 | tee perf_analyzer_help.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then measure the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "perf_analyzer -m hifigan --shape \"spec:80,140\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command above is again using the inefficient HTTP. The optimal launch will use GRPC, shared memory, batch size != 1 and several streams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "perf_analyzer -m hifigan --shape \"spec:80,140\" \\\n",
    "-b 4 \\\n",
    "-i gRPC \\\n",
    "--concurrency-range 1:3 \\\n",
    "--shared-memory \"cuda\" \\\n",
    "--output-shared-memory-size 60000000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a V100 the performance gain is 30%. But which set of hyperparameters is the most optimal? Model Analyzer to the rescue!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Analyzer Launch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Analyzer is used to select the optimal model cofig both for offline and online modes. To do it, it creates models with various configurations, launches the Triton container and uses Perf Analyzer to measure the performance. Model Analyzer is [Open Source](https://github.com/triton-inference-server/model_analyzer) and written in Python.\n",
    "\n",
    "It is advised to go through this Notebook in the latest Triton SDK container. One should mount the full path to the Notebook by the similar path inside the container, so that the model analyzer could mount it again to the Triton container. If one launches the container from the path, where this Notebook is, it is recommended to launch it like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker run --rm --gpus '\"device=0\"' -it --ipc=host \\\n",
    "-v $HOME/:/ext_home \\\n",
    "-v /var/run/docker.sock:/var/run/docker.sock \\\n",
    "-v ${PWD}:${PWD} \\\n",
    "-w ${PWD} \\\n",
    "--net=host \\\n",
    "--name ${USER}_triton_sdk \\\n",
    "nvcr.io/nvidia/tritonserver:22.12-py3-sdk \\\n",
    "/bin/bash"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To launch the Notebook from the container, one should additionally install `ipykernel`, but it may turn out to be simpler just to copy all the next coommands to the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install ipykernel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the docker.sock mount, which enables model_analyzer launch containers from woithin other containers.\n",
    "\n",
    "If the access to the inference machine is available only through Kubernetes, it is also [supported](https://github.com/triton-inference-server/model_analyzer/blob/main/docs/kubernetes_deploy.md), but is out of scope for this demo.\n",
    "\n",
    "Before launching Model Analyzer, one must make sure the server has no other GPU containers running. Otherwise, the results would be skewed. To achieve this, we should kill our previous Triton container, if it is still running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker rm -f tritonserver "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Analyzer Config\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the measurable models one should create a standard Triton model repo. In our `model_repository` directory there is one hifigan model in the signle version: 1. \n",
    "\n",
    "\n",
    "Model Analyzer has several modes of hyperparameter search: \n",
    "\n",
    "* [Automatic Brute Search](https://github.com/triton-inference-server/model_analyzer/blob/main/docs/config_search.md#automatic-brute-search)\n",
    "* [Manual Brute Search](https://github.com/triton-inference-server/model_analyzer/blob/main/docs/config_search.md#manual-brute-search)\n",
    "* and [Quick Search Mode](https://github.com/triton-inference-server/model_analyzer/blob/main/docs/config_search.md#quick-search-mode)\n",
    "\n",
    "Model Analyzer's **brute search mode** will do a brute-force sweep of the cross product of all possible configurations. \n",
    "\n",
    "**Automatic brute** configuration search is the default behavior when running Model Analyzer without manually specifying what values to search. The parameters that are automatically searched are max_batch_size and instance_group. Additionally, dynamic_batching will be enabled if it is legal to do so.\n",
    "\n",
    "Using **manual config search**, you can create custom sweeps for every parameter that can be specified in the model configuration. Model Analyzer only checks the syntax of the model_config_parameters that is specified and cannot guarantee that the configuration that is generated is loadable by Triton.\n",
    "\n",
    "[See the CLI docs for reference](https://github.com/triton-inference-server/model_analyzer/blob/main/docs/cli.md#subcommand-profile)\n",
    "\n",
    "We will use the manual config search to speed up the demo. Please, examine the config file [profile_config_manual.yaml](profile_config_manual.yaml)\n",
    "\n",
    "Note the shapes specified in config.\n",
    "\n",
    "The next command creates an `export_path` and launches Model Analyzer with the prepared manual config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "mkdir -p analyzer_export && model-analyzer profile -f profile_config_manual.yaml && echo -e \"\\07\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to pause the command execution, you can send it `SIGINT` signal. One can simply do it via `Ctrl+C`, pressing the stop sign next to the cell or run from another terminal within the same container"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kill -INT $(ps aux | grep model-ana | grep python | sed \"s/^[[:alnum:]]*[[:space:]]*\\([[:digit:]]*\\).*/\\1/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the log should read like\n",
    "```\n",
    "INFO[analyzer_state_manager.py:174] Received SIGINT 1/3. Will attempt to exit after current measurement.\n",
    "```\n",
    "This means model-analyzer is waiting for the completion of the current measurement, before checkpointing all the results. This checkpoint will already allow for preliminary analysis (see below)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the profiling is completed, one should build the reports. It's very fast and doesn't induce any GPU load. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!model-analyzer report -f profile_config_manual.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most interesting results one should be able to find in `analyzer_export/reports/summaries/hifigan/results_summary.pdf`.\\\n",
    "If you're just browsing, see the examples of the report available for another model in the [Model Analyzer/examples](https://github.com/triton-inference-server/model_analyzer/tree/main/examples)\n",
    "\n",
    "If you use Jupyter, the reports are available for you to view from the browser\n",
    "\n",
    "[analyzer_export/reports/summaries/hifigan/results_summary.pdf](analyzer_export/reports/summaries/hifigan/results_summary.pdf)\n",
    "\n",
    "The details are in the csv files\n",
    "[analyzer_export/results/metrics-model-gpu.csv](analyzer_export/results/metrics-model-gpu.csv)\n",
    "\n",
    "[analyzer_export/results/metrics-model-inference.csv](analyzer_export/results/metrics-model-inference.csv)\n",
    "\n",
    "[analyzer_export/results/metrics-server-only.csv](analyzer_export/results/metrics-server-only.csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b1ae6075174f827178c6feee5c7e607363c2b4d0e2c70d97afa0b2cad99abdff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
