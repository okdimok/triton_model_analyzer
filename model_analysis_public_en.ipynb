{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPDX-FileCopyrightText: Copyright (c) 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "\n",
    "SPDX-License-Identifier: MIT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Triton Model Analyzer\n",
    "\n",
    "## Assumptions\n",
    "You have an access to host, where you can run docker containers. The host is connected to the Internet."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your model repository\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next commands to work, it's important to clone this repository to the filesystem, which supports symbollic links, transparently for docker. Any linux disk partition is sufficient. NFS and NTFS are not."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we're using [Hi-Fi GAN](https://ngc.nvidia.com/catalog/models/nvidia:nemo:tts_hifigan) model from [NeMo](https://github.com/NVIDIA/NeMo).\n",
    "To get it's ONNX one should run the NeMo container in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker run --rm --gpus '\"device=0\"' -it --ipc=host \\\n",
    "-v $HOME/:/ext_home \\\n",
    "-v ${PWD}:${PWD} \\\n",
    "-w ${PWD} \\\n",
    "--name ${USER}_nemo \\\n",
    "nvcr.io/nvidia/nemo:1.3.0 \\\n",
    "-- python get_hifigan.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The command we run inside the container\n",
    "```\n",
    "python get_hifigan.py\n",
    "```\n",
    "is equivalent to the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.models import HifiGanModel\n",
    "\n",
    "model = HifiGanModel.from_pretrained(model_name=\"tts_hifigan\")\n",
    "model.export(\"./hifigan.onnx\")\n",
    "\n",
    "model = HifiGanModel.from_pretrained(model_name=\"tts_hifigan\")\n",
    "model.export(\"./hifigan.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the cell above, two files are to appear in the current directory: `hifigan.onnx` and `hifigan.pt`\n",
    "\n",
    "We'll need the ONNX model to experiment with Model Analyzer — this is the tool, that helps select the optimal inference config, within the specific backrnd. We need to copy  `hifigan.onnx` to `model_repository/hifigan/1/model.onnx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "mkdir -p model_repository/hifigan/1\n",
    "cp hifigan.onnx model_repository/hifigan/1/model.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "TorchScript will be required later, for the Model Navigator experiments. It help in selecting the most optimal backend fot the specific model.\n",
    "Having these files, the NeMo container can be stopped. For this, it's sufficient to exit its shell due to the `--rm` flag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curl and Perf Analyzer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick Triton test launch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker run --rm --gpus '\"device=0\"' -it --ipc=host \\\n",
    "-v $HOME/:/ext_home \\\n",
    "-v ${PWD}:${PWD} \\\n",
    "-w ${PWD} \\\n",
    "-p 8000:8000 \\\n",
    "-p 8001:8001 \\\n",
    "-p 8002:8002 \\\n",
    "--name ${USER}_triton \\\n",
    "nvcr.io/nvidia/tritonserver:22.12-py3 \\\n",
    "tritonserver --model-repository ${PWD}/model_repository --log-verbose 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Note, that such amount of logs can negatively affect the performance, and is recommended for debug only.\n",
    "\n",
    "After this, one can run in another terminal to quickly check, if the server works. One should expect long json as an output of the command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "curl -kv -X POST 'http://127.0.0.1:8000/v2/models/hifigan/infer' \\\n",
    " -H 'accept: application/json' \\\n",
    " -H 'Content-Type: application/octet-stream' \\\n",
    " -H 'connection: keep-alive' \\\n",
    " -d @hifigan_curl_data.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note, that it uses an HTTP protocol, which has quite high overhead.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now one can launch Triton SDK container in yet another terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker run --rm --gpus '\"device=0\"' -it --ipc=host \\\n",
    "-v $HOME/:/ext_home \\\n",
    "-v ${PWD}:${PWD} \\\n",
    "-w ${PWD} \\\n",
    "--net=host \\\n",
    "--name ${USER}_triton_sdk \\\n",
    "nvcr.io/nvidia/tritonserver:22.12-py3-sdk \\\n",
    "/bin/bash"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can save perf_analyzer help for later usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "perf_analyzer --help 2>&1 | tee perf_analyzer_help.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then measure the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "perf_analyzer -m hifigan --shape \"spec:80,140\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command above is again using the inefficient HTTP. The optimal launch will use GRPC, shared memory, batch size != 1 and several streams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "perf_analyzer -m hifigan --shape \"spec:80,140\" \\\n",
    "-b 4 \\\n",
    "-i gRPC \\\n",
    "--concurrency-range 1:3 \\\n",
    "--shared-memory \"cuda\" \\\n",
    "--output-shared-memory-size 60000000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a V100 the performance gain is 30%. But which set of hyperparameters is the most optimal? Model Analyzer to the rescue!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Analyzer Launch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Analyzer is used to select the optimal model cofig both for offline and online modes. To do it, it creates models with various configurations, launches the Triton container and uses Perf Analyzer to measure the performance. Model Analyzer is [Open Source](https://github.com/triton-inference-server/model_analyzer) and written in Python.\n",
    "\n",
    "It is advised to go through this Notebook in the latest Triton SDK container. One should mount the full path to the Notebook by the similar path inside the container, so that the model analyzer could mount it again to the Triton container. If one launches the container from the path, where this Notebook is, it is recommended to launch it like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker run --rm --gpus '\"device=0\"' -it --ipc=host \\\n",
    "-v $HOME/:/ext_home \\\n",
    "-v /var/run/docker.sock:/var/run/docker.sock \\\n",
    "-v ${PWD}:${PWD} \\\n",
    "-w ${PWD} \\\n",
    "--net=host \\\n",
    "--name ${USER}_triton_sdk \\\n",
    "nvcr.io/nvidia/tritonserver:22.12-py3-sdk \\\n",
    "/bin/bash"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To launch the Notebook from the container, one should additionally install `ipykernel`, but it may turn out to be simpler just to copy all the next coommands to the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install ipykernel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание на подмонтированный docker.sock, который позволяет запускать контейнеры.\n",
    "\n",
    "Если доступ на машину с GPU для инференса есть только через Kubernetes, это [тоже поддерживается](https://github.com/triton-inference-server/model_analyzer/blob/main/docs/kubernetes_deploy.md), но я не буду на этом останавливаться.\n",
    "\n",
    "Перед запуском команд на тестирование следует убедиться, что на машине сейчас нет запущенного контейнера с name=tritonserver. Следующая команда остановит и удалит такой контейнер, если он был."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker rm -f tritonserver "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Analyzer Config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из моделей, с которыми будет работать model analyzer, необходимо создать стандартный репозиторий моделей для Triton. У меня в репозитории в папке `model_repository` одна модель hifigan в одной версии: 1. \n",
    "\n",
    "Есть два режима подбора параметров Model Analyzer: \n",
    "[автоматический](https://github.com/triton-inference-server/model_analyzer/blob/main/docs/config_search.md#automatic-configuration-search)\n",
    "и [ручной](https://github.com/triton-inference-server/model_analyzer/blob/main/docs/config_search.md#Manual-Configuration-Search)\n",
    "— [введение в команду model-analyzer profile](https://github.com/triton-inference-server/model_analyzer/blob/main/docs/cli.md#subcommand-profile)\n",
    "\n",
    "Я подготовил для Hi-Fi GAN два конфига: ручной [profile_config_manual.yaml](profile_config_manual.yaml) и\n",
    "автоматический [profile_config_auto.yaml](profile_config_auto.yaml). Они отличаются значением параметра `run_config_search_disable`. Больше подробностей о других параметрах [тут](https://github.com/triton-inference-server/model_analyzer/blob/main/docs/config.md#configuring-model-analyzer).\n",
    "\n",
    "**В ручном конфиге**  Измеряются задержки и пропускная способность Hi-FI GAN на различных парметрах размера батча и одновременных запросов, и при различном количестве инстансов.\n",
    "\n",
    "**В автоматическом конфиге** эти же параметры, а так же наличие динамического батчинга, подбираются в автоматическом режиме.\n",
    "\n",
    "**В обоих конфигах необходимо задать полный путь до выходного репозитория моделей**, в котором можно будет посмотреть измеряемые конфиги моделей. \n",
    "\n",
    "Обратите внимание на задание шейпов в конфиге.\n",
    "\n",
    "Следующая команда запускает Model Analyzer с ручным конфигом. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "mkdir -p analyzer_export && model-analyzer profile -f profile_config_manual.yaml && echo -e \"\\07\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если захочется приостановить выполнение команды, можно послать ей сигнал `SIGINT`. Это можно сделать либо нажав `Ctrl+C`, либо нажав на знак «стоп» рядом с ячейкой, либо (самое надёжное), выполнить в терминале внутри контейнера\n",
    "\n",
    "```\n",
    "kill -INT $(ps aux | grep model-ana | grep python | sed \"s/^[[:alnum:]]*[[:space:]]*\\([[:digit:]]*\\).*/\\1/\")\n",
    "```\n",
    "\n",
    "При этом в логе должно появиться\n",
    "```\n",
    "INFO[analyzer_state_manager.py:174] Received SIGINT 1/3. Will attempt to exit after current measurement.\n",
    "```\n",
    "\n",
    "Это означает, что model-analyzer дождётся окончания текущего измерения и после этого сохранит текущий статус в checkpoint, из которого можно уже сделать предварительный анализ (см. дальше)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**После завершения профилировки** необходимо запустить анализ. Он выполнится очень быстро, никакой нагрузки на GPU он не создаёт.\n",
    "Самые интересные результаты окажутся в папке results (сейчас там лежат пример результатов на NVIDIA A10). PDF-отчёт будет лежать в папке reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!model-analyzer report -f profile_config_manual.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты будут выведены в терминал, общий анализ в PDF:\n",
    "\n",
    "[reports/summaries/hifigan/result_summary.pdf](reports/summaries/hifigan/result_summary.pdf)\n",
    "\n",
    "И подробности в csv файлы:\n",
    "\n",
    "[results/metrics-model-gpu.csv](results/metrics-model-gpu.csv)\n",
    "\n",
    "[results/metrics-model-inference.csv](results/metrics-model-inference.csv)\n",
    "\n",
    "[results/metrics-server-only.csv](results/metrics-server-only.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# чтобы верифицировать yaml синтаксически\n",
    "import yaml\n",
    "with open(\"profile_config_manual.yaml\") as pc:\n",
    "    pc = yaml.safe_load(pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Navigator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Navigator не установлен в контейнере с Triton SDK. Его контейнер необходимо собрать самостоятельно из исходников. [Инструкция.](https://github.com/triton-inference-server/model_navigator/blob/main/docs/quick_start.md#install-the-triton-model-navigator-and-run-container)\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/triton-inference-server/model_navigator.git\n",
    "# Optional\n",
    "# git checkout v0.2.4\n",
    "make docker\n",
    "cd ..\n",
    "\n",
    "docker run -it --rm \\\n",
    " --gpus 1 \\\n",
    " -v /var/run/docker.sock:/var/run/docker.sock \\\n",
    " -v ${PWD}:${PWD} \\\n",
    " -w ${PWD} \\\n",
    " --net host \\\n",
    " --name model-navigator \\\n",
    " model-navigator /bin/bash\n",
    "```\n",
    "\n",
    "Конфигурация для Model Navigator хранится в файле [navigator_config.yaml](navigator_config.yaml). При работе Model Navigator склонен перезаписывать этот файл, поэтому команда запуска внутри контейнера предлагается такая:\n",
    "\n",
    "```\n",
    "cp navigator_config.yaml navigator_config_run.yaml; model-navigator run --config-path navigator_config_run.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b1ae6075174f827178c6feee5c7e607363c2b4d0e2c70d97afa0b2cad99abdff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
