These are the examples for the webinar on Triton Inference Server.
For any additional information, feel free to contact 
Dmitry Mironov [dmitrym@nvidia.com](mailto:dmitrym@nvidia.com)

This repository contains three demos:
* Model Analyzer demo [in English](model_analysis_public_en.ipynb) and [in Russian](model_analysis_public.ipynb)
* [Basic TensorRT + Triton demo (English)](triton_webinar/README.md) + [On-demand recording](https://info.nvidia.com/indepth-review-triton-inference-webinar.html?ondemandrgt=yes#)
* [PyTriton and Some BLS](triton_webinar_pytriton_model_analyzer/README.md) + [On-demand recording](https://info.nvidia.com/Dec-EMEA-Inference-Webinar.html)
* [Work In Progress] Triton in Kubernetes demo [English only](Triton_in_Kubernetes_en.ipynb)


Additional file is [onnx-to-triton.ipynb](onnx-to-triton.ipynb), it can be used to generated a basic Triton config.pbtxt file from an ONNX file.

SPDX-FileCopyrightText: Copyright (c) 2021 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
SPDX-License-Identifier: MIT
Please, see the [LICENSE](LICENSE) file.