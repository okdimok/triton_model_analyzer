# SPDX-FileCopyrightText: Copyright (c) 2021 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: MIT


model_repository: model_repository
# One must make sure the full path to the output_model_repository allows to mount it to a child container (i.e. it is the same inside the container and outside it):
# prior to 22.01 one must specify the full path
output_model_repository_path: /raid/users/dmitrym/triton_model_analyzer_clone/output_model_repository
triton_launch_mode: docker
# override_output_model_repository: True
run_config_search_disable: True

# https://github.com/triton-inference-server/model_analyzer/blob/main/docs/metrics.md
inference_output_fields: [
  'model_name', 'batch_size', 'concurrency', 'model_config_path',
  'instance_group', 'dynamic_batch_sizes', 'satisfies_constraints',
  'perf_throughput', 'perf_latency_p99',
  'perf_client_response_wait', perf_client_send_recv, perf_server_queue,
  perf_server_compute_input, perf_server_compute_infer, perf_server_compute_output
]

#https://github.com/triton-inference-server/model_analyzer/blob/main/docs/config.md#configuring-model-analyzer
profile_models:
  hifigan:
    parameters:
      concurrency: [1, 3]
      batch_sizes: [1, 4, 8]
    perf_analyzer_flags:
      shape: 
      - "spec:80,140"
    model_config_parameters:
      version_policy: 
        specific:
          versions: [[1]]
      instance_group:
      -
        kind: KIND_GPU
        count: [1, 2, 4]

analysis_models: hifigan
 


