model_repository: model_repository
# One must specify the full path to the output_model_repository here:
output_model_repository_path: /raid/users/dmitrym/triton_model_analyzer/output_model_repository_
triton_launch_mode: docker
override_output_model_repository: False
run_config_search_disable: True

# https://github.com/triton-inference-server/model_analyzer/blob/main/docs/metrics.md
inference_output_fields: [
  'model_name', 'batch_size', 'concurrency', 'model_config_path',
  'instance_group', 'dynamic_batch_sizes', 'satisfies_constraints',
  'perf_throughput', 'perf_latency_p99',
  'perf_client_response_wait', perf_client_send_recv, perf_server_queue,
  perf_server_compute_input, perf_server_compute_infer, perf_server_compute_output
]

profile_models:
  hifigan:
    parameters:
      concurrency: [1, 3]
      batch_sizes: [1, 4, 8]
    perf_analyzer_flags:
      shape: "spec:80,140"
    model_config_parameters:
      version_policy: 
        specific:
          versions: [[1]]
      instance_group:
      -
        kind: KIND_GPU
        count: [1, 2, 4]

analysis_models: hifigan
 


