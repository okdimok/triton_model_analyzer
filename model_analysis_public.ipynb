{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPDX-FileCopyrightText: Copyright (c) 2021 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "\n",
    "SPDX-License-Identifier: MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка репозитория моделей\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для последующих команд важно, чтобы вы склонировали этот репосзиторий на файловую систему, которая прозрачно для docker поддерживает символьные ссылки. Подходит любой линуксовый раздел диска. Не подходит NFS и, возможно, NTFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве примера будет использовать модель [Hi-Fi GAN](https://ngc.nvidia.com/catalog/models/nvidia:nemo:tts_hifigan) из [NeMo](https://github.com/NVIDIA/NeMo).\n",
    "Чтобы получить onnx этой модели нужно запустить в текущей директории контейнер NeMo\n",
    "```\n",
    "docker run --rm --gpus '\"device=0\"' -it --ipc=host \\\n",
    "-v $HOME/:/ext_home \\\n",
    "-v ${PWD}:${PWD} \\\n",
    "-w ${PWD} \\\n",
    "--name ${USER}_nemo \\\n",
    "nvcr.io/nvidia/nemo:1.3.0\n",
    "```\n",
    "\n",
    "И в нём выполнить следующую ячейку или команду\n",
    "```\n",
    "python get_hifigan.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.models import HifiGanModel\n",
    "\n",
    "model = HifiGanModel.from_pretrained(model_name=\"tts_hifigan\")\n",
    "model.export(\"./hifigan.onnx\")\n",
    "\n",
    "model = HifiGanModel.from_pretrained(model_name=\"tts_hifigan\")\n",
    "model.export(\"./hifigan.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После исполнения ячейки сверху в текущей директории появятся два файла `hifigan.onnx` и `hifigan.pt`\n",
    "\n",
    "ONNX модель потребуется для экспериментов с Model Analyzer — этот инструмент помогает подобрать оптимальную конфигурацию для инференса в рамках одного бекенда. \n",
    "Для этого нужно скопировать `hifigan.onnx` в `model_repository/hifigan/1/model.onnx`\n",
    "\n",
    "```\n",
    "cp hifigan.onnx model_repository/hifigan/1/model.onnx\n",
    "```\n",
    "\n",
    "TorchScript модель понадобится позже, для экспериментов с Model Navigator, который может помочь подобрать наиболее оптимальный бекенд для модели.\n",
    "После этого, контейнер с NeMo можно остановить. Для этого достаточно выйти из него."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curl and Perf Analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бытрый тестовый запуск triton\n",
    "```\n",
    "docker run --rm --gpus '\"device=0\"' -it --ipc=host \\\n",
    "-v $HOME/:/ext_home \\\n",
    "-v ${PWD}:${PWD} \\\n",
    "-w ${PWD} \\\n",
    "-p 8000:8000 \\\n",
    "-p 8001:8001 \\\n",
    "-p 8002:8002 \\\n",
    "--name ${USER}_triton \\\n",
    "nvcr.io/nvidia/tritonserver:21.11-py3 \\\n",
    "tritonserver --model-repository ${PWD}/model_repository --log-verbose 4\n",
    "```\n",
    "\n",
    "Обратите внимание, что такое большое количество логов может влиять на производительность и рекомендуется только для отладки.\n",
    "\n",
    "После этого в другом терминале можно запустить\n",
    "```\n",
    "curl -kv -X POST 'http://127.0.0.1:8000/v2/models/hifigan/infer' \\\n",
    " -H 'accept: application/json' \\\n",
    " -H 'Content-Type: application/octet-stream' \\\n",
    " -H 'connection: keep-alive' \\\n",
    " -d @hifigan_curl_data.json\n",
    "```\n",
    "\n",
    "Обратите внимание, что выше используется неэффективный протокол HTTP.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно запустить контейнер с клиентами Triton в другом терминале:\n",
    "```\n",
    "docker run --rm --gpus '\"device=0\"' -it --ipc=host \\\n",
    "-v $HOME/:/ext_home \\\n",
    "-v ${PWD}:${PWD} \\\n",
    "-w ${PWD} \\\n",
    "--net=host \\\n",
    "--name ${USER}_triton_sdk \\\n",
    "nvcr.io/nvidia/tritonserver:21.11-py3-sdk \\\n",
    "/bin/bash\n",
    "```\n",
    "\n",
    "В нём можно сохранить себе help perf_analyzer для удобного посмотра\n",
    "```\n",
    "perf_analyzer --help 2>&1 | tee perf_analyzer_help.txt\n",
    "```\n",
    "\n",
    "А после этого измерить производительность модели\n",
    "\n",
    "```\n",
    "perf_analyzer -m hifigan --shape \"spec:80,140\"\n",
    "```\n",
    "\n",
    "Команда выше снова использует HTTP. Оптимальный запуск с использованием GRPC, shared memory, batch size != 1 и нескольких потоков:\n",
    "\n",
    "```\n",
    "perf_analyzer -m hifigan --shape \"spec:80,140\" \\\n",
    "-b 4 \\\n",
    "-i gRPC \\\n",
    "--concurrency-range 1:3 \\\n",
    "--shared-memory \"cuda\" \\\n",
    "--output-shared-memory-size 60000000\n",
    "```\n",
    "\n",
    "На V100 различие в производительности — 30%. Но какие же гиперпараметры самые оптимальные? Model Analyzer в помощь!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Analyzer Launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Model Analyzer предназначен для выбора оптимальной конфигурации моделей для онлайн или оффлайн режима. Для этого он создаёт модели с различными конфигурациями, самостоятельно запускает контейнер с Triton'ом и использует Perf Analyzer для снятия метрик. Model Analyzer [Open Source](https://github.com/triton-inference-server/model_analyzer) и написан на Python.\n",
    "\n",
    "Этот ноутбук рекомендуется исполнять в последнем контейнере с Triton SDK. При этом нужно подмонтировать полный путь до этого ноутбука под таким же путём, как и снаружи, чтобы model_analyzer мог удобно подмонтировать его к контейнеру с Triton'ом. Если запускать контейнер в папке с этим ноутбуком, рекомендуемая команда запуска\n",
    "\n",
    "```\n",
    "docker run --rm --gpus '\"device=0\"' -it --ipc=host \\\n",
    "-v $HOME/:/ext_home \\\n",
    "-v /var/run/docker.sock:/var/run/docker.sock \\\n",
    "-v ${PWD}:${PWD} \\\n",
    "-w ${PWD} \\\n",
    "--net=host \\\n",
    "--name ${USER}_triton_sdk \\\n",
    "nvcr.io/nvidia/tritonserver:21.11-py3-sdk \\\n",
    "/bin/bash\n",
    "```\n",
    "\n",
    "Для запуска ноутбука в контейнере можно дополнительно установить ipykernel, а можно просто копировать все последующие команды в терминал.\n",
    "```\n",
    "pip install ipykernel\n",
    "```\n",
    "\n",
    "Обратите внимание на подмонтированный docker.sock, который позволяет запускать контейнеры.\n",
    "\n",
    "Если доступ на машину с GPU для инференса есть только через Kubernetes, это [тоже поддерживается](https://github.com/triton-inference-server/model_analyzer/blob/main/docs/kubernetes_deploy.md), но я не буду на этом останавливаться.\n",
    "\n",
    "Перед запуском команд на тестирование следует убедиться, что на машине сейчас нет запущенного контейнера с name=tritonserver. Следующая команда остановит и удалит такой контейнер, если он был.\n",
    "```\n",
    "docker rm -f tritonserver \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Analyzer Config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из моделей, с которыми будет работать model analyzer, необходимо создать стандартный репозиторий моделей для Triton. У меня в репозитории в папке `model_repository` одна модель hifigan в одной версии: 1. \n",
    "\n",
    "Есть два режима подбора параметров Model Analyzer: \n",
    "[автоматический](https://github.com/triton-inference-server/model_analyzer/blob/main/docs/config_search.md#automatic-configuration-search)\n",
    "и [ручной](https://github.com/triton-inference-server/model_analyzer/blob/main/docs/config_search.md#Manual-Configuration-Search)\n",
    "— [введение в команду model-analyzer profile](https://github.com/triton-inference-server/model_analyzer/blob/main/docs/cli.md#subcommand-profile)\n",
    "\n",
    "Я подготовил для Hi-Fi GAN два конфига: ручной [profile_config_manual.yaml](profile_config_manual.yaml) и\n",
    "автоматический [profile_config_auto.yaml](profile_config_auto.yaml). Они отличаются значением параметра `run_config_search_disable`. Больше подробностей о других параметрах [тут](https://github.com/triton-inference-server/model_analyzer/blob/main/docs/config.md#configuring-model-analyzer).\n",
    "\n",
    "**В ручном конфиге**  Измеряются задержки и пропускная способность Hi-FI GAN на различных парметрах размера батча и одновременных запросов, и при различном количестве инстансов.\n",
    "\n",
    "**В автоматическом конфиге** эти же параметры, а так же наличие динамического батчинга, подбираются в автоматическом режиме.\n",
    "\n",
    "**В обоих конфигах необходимо задать полный путь до выходного репозитория моделей**, в котором можно будет посмотреть измеряемые конфиги моделей. \n",
    "\n",
    "Обратите внимание на задание шейпов в конфиге.\n",
    "\n",
    "Следующая команда запускает Model Analyzer с ручным конфигом. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!model-analyzer profile -f profile_config_manual.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вы видите ошибку \n",
    "```\n",
    "2021-12-11 12:42:17.115 ERROR[entrypoint.py:214] Model Analyzer encountered an error: Failed to set the value for field \"triton_server_path\". Error: Either the binary 'tritonserver' is not on the PATH, or Model Analyzer does not have permissions to execute os.stat on this path. \n",
    "```\n",
    "\n",
    "Запустите\n",
    "```\n",
    "mkdir -p /opt/tritonserver\n",
    "touch /workspace/install/bin/tritonserver\n",
    "chmod u+x /workspace/install/bin/tritonserver\n",
    "```\n",
    "В версии 21.11 есть ошибка чрезмерной валидации входных параметров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если захочется приостановить выполнение команды, можно послать ей сигнал `SIGINT`. Это можно сделать либо нажав `Ctrl+C`, либо нажав на знак «стоп» рядом с ячейкой, либо (самое надёжное), выполнить в терминале внутри контейнера\n",
    "\n",
    "```\n",
    "kill -INT $(ps aux | grep model-ana | grep python | sed \"s/^[[:alnum:]]*[[:space:]]*\\([[:digit:]]*\\).*/\\1/\")\n",
    "```\n",
    "\n",
    "При этом в логе должно появиться\n",
    "```\n",
    "INFO[analyzer_state_manager.py:174] Received SIGINT 1/3. Will attempt to exit after current measurement.\n",
    "```\n",
    "\n",
    "Это означает, что model-analyzer дождётся окончания текущего измерения и после этого сохранит текущий статус в checkpoint, из которого можно уже сделать предварительный анализ (см. дальше)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**После завершения профилировки** необходимо запустить анализ. Он выполнится очень быстро, никакой нагрузки на GPU он не создаёт.\n",
    "Самые интересные результаты окажутся в папке results (сейчас там лежат пример результатов на NVIDIA A10). PDF-отчёт будет лежать в папке reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!model-analyzer analyze -f profile_config_manual.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# чтобы верифицировать yaml синтаксически\n",
    "import yaml\n",
    "with open(\"profile_config_manual.yaml\") as pc:\n",
    "    pc = yaml.safe_load(pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Navigator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Navigator не установлен в контейнере с Triton SDK. Его контейнер необходимо собрать самостоятельно из исходников. [Инструкция.](https://github.com/triton-inference-server/model_navigator/blob/main/docs/quick_start.md#install-the-triton-model-navigator-and-run-container)\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/triton-inference-server/model_navigator.git\n",
    "# Optional\n",
    "# git checkout v0.2.2\n",
    "make docker\n",
    "cd ..\n",
    "\n",
    "docker run -it --rm \\\n",
    " --gpus 1 \\\n",
    " -v /var/run/docker.sock:/var/run/docker.sock \\\n",
    " -v ${PWD}:${PWD} \\\n",
    " -w ${PWD} \\\n",
    " --net host \\\n",
    " --name model-navigator \\\n",
    " model-navigator /bin/bash\n",
    "```\n",
    "\n",
    "Конфигурация для Model Navigator хранится в файле [navigator_config.yaml](navigator_config.yaml). При работе Model Navigator склонен перезаписывать этот файл, поэтому команда запуска внутри контейнера предлагается такая:\n",
    "\n",
    "```\n",
    "cp navigator_config.yaml navigator_config_run.yaml; model-navigator run --config-path navigator_config_run.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
