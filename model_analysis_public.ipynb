{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPDX-FileCopyrightText: 2021 NVIDIA CORPORATION\n",
    "\n",
    "SPDX-License-Identifier: MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка репозитория моделей\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве примера будет использовать модель [Hi-Fi GAN](https://ngc.nvidia.com/catalog/models/nvidia:nemo:tts_hifigan) из [NeMo](https://github.com/NVIDIA/NeMo).\n",
    "Чтобы получить onnx этой модели нужно запустить контейнер NeMo\n",
    "```\n",
    "docker run --rm --gpus '\"device=0\"' -it --ipc=host \\\n",
    "-v $HOME/:/ext_home \\\n",
    "-v ${PWD}:${PWD} \\\n",
    " -w ${PWD} \\\n",
    "--name $USER_nemo \\\n",
    "nvcr.io/nvidia/nemo:1.3.0\n",
    "```\n",
    "\n",
    "И в нём выполнить следующую ячейку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.models import HifiGanModel\n",
    "\n",
    "model = HifiGanModel.from_pretrained(model_name=\"tts_hifigan\")\n",
    "model.export(\"./hifigan.onnx\")\n",
    "\n",
    "model = HifiGanModel.from_pretrained(model_name=\"tts_hifigan\")\n",
    "model.export(\"./hifigan.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После исполнения ячейки сверху в текущей директории появятся два файла `hifigan.onnx` и `hifigan.pt`\n",
    "\n",
    "ONNX модель потребуется для экспериментов с Model Analyzer — этот инструмент помогает подобрать оптимальную конфигурацию для инференса в рамках одного бекенда. \n",
    "Для этого нужно скопировать `hifigan.onnx` в `model_repository/hifigan/1/model.onnx`\n",
    "\n",
    "```\n",
    "cp hifigan.onnx model_repository/hifigan/1/model.onnx\n",
    "```\n",
    "\n",
    "TorchScript модель понадобится позже, для экспериментов с Model Navigator, который может помочь подобрать наиболее оптимальный бекенд для модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Analyzer Launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Model Analyzer предназначен для выбора оптимальной конфигурации моделей для онлайн или оффлайн режима. Для этого он создаёт модели с различными конфигурациями, самостоятельно запускает контейнер с Triton'ом и использует Perf Analyzer для снятия метрик. Model Analyzer [Open Source](https://github.com/triton-inference-server/model_analyzer) и написан на Python.\n",
    "\n",
    "Этот ноутбук рекомендуется исполнять в последнем контейнере с Triton SDK. При этом нужно подмонтировать полный путь до этого ноутбука под таким же путём, как и снаружи, чтобы model_analyzer мог удобно подмонтировать его к контейнеру с Triton'ом. Если запускать контейнер в папке с этим ноутбуком, рекомендуемая команда запуска\n",
    "\n",
    "```\n",
    "docker run --rm --gpus '\"device=0\"' -it --ipc=host --net=host \\\n",
    "-v /var/run/docker.sock:/var/run/docker.sock \\\n",
    "-v $(pwd):$(pwd) \\\n",
    "-v $HOME/:/ext_home \\\n",
    "--name $USER_triton_sdk \\\n",
    "nvcr.io/nvidia/tritonserver:21.08-py3-sdk\n",
    "```\n",
    "\n",
    "Для запуска ноутбука в контейнере необходимо дополнительно установить ipykernel\n",
    "```\n",
    "pip install ipykernel\n",
    "```\n",
    "\n",
    "Обратите внимание на подмонтированный docker.sock, который позволяет запускать контейнеры.\n",
    "\n",
    "Если доступ на машину с GPU для инференса есть только через Kubernetes, это [тоже поддерживается](https://github.com/triton-inference-server/model_analyzer/blob/main/docs/kubernetes_deploy.md), но я не буду на этом останавливаться.\n",
    "\n",
    "Перед запуском следующей команды следует убедиться, что на машине сейчас нет запущенного контейнера с name=tritonserver. Следующая команда остановит и удалит такой контейнер, если он был.\n",
    "```\n",
    "docker rm -f tritonserver \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Analyzer Config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из моделей, с которыми будет работать model analyzer, необходимо создать стандартный репозиторий моделей для Triton. У меня в репозитории в папке `model_repository` одна модель hifigan в двух версиях: 1 и 2. Версия 1 у меня напрямую экспортирована из NeMo, в версии 2 я вырезал бесполезный подграф. Сами файлы не прикладываю, чтобы не раздувать архив.\n",
    "\n",
    "Есть два режима подбора параметров Model Analyzer: \n",
    "[автоматический](https://github.com/triton-inference-server/model_analyzer/blob/main/docs/config_search.md#automatic-configuration-search)\n",
    "и [ручной](https://github.com/triton-inference-server/model_analyzer/blob/main/docs/config_search.md#Manual-Configuration-Search)\n",
    "— [введение в команду model-analyzer profile](https://github.com/triton-inference-server/model_analyzer/blob/main/docs/cli.md#subcommand-profile)\n",
    "\n",
    "Я подготовил для Hi-Fi GAN два конфига: ручной [profile_config_manual.yaml](profile_config_manual.yaml) и\n",
    "автоматический [profile_config_auto.yaml](profile_config_auto.yaml). Они отличаются значением параметра `run_config_search_disable`. Больше подробностей о других параметрах [тут](https://github.com/triton-inference-server/model_analyzer/blob/main/docs/config.md#configuring-model-analyzer).\n",
    "\n",
    "**В ручном конфиге**  Измеряются задержки и пропускная способность Hi-FI GAN на различных парметрах размера батча и одновременных запросов, и при различном количестве инстансов.\n",
    "\n",
    "**В автоматическом конфиге** эти же параметры, а так же наличие динамического батчинга, подбираются в автоматическом режиме.\n",
    "\n",
    "**В обоих конфигах необходимо задать полный путь до выходного репозитория моделей**, в котором можно будет посмотреть измеряемые конфиги моделей. \n",
    "\n",
    "Обратите внимание на задание шейпов в конфиге.\n",
    "\n",
    "Следующая команда запускает Model Analyzer с ручным конфигом. Если захочется приостановить выполнение команды, можно послать ей сигнал `SIGINT`. Это можно сделать либо нажав `Ctrl+C`, либо нажав на знак «стоп» рядом с ячейкой, либо (самое надёжное), выполнить в терминале внутри контейнера\n",
    "\n",
    "```\n",
    "kill -INT $(ps aux | grep model-ana | grep python | sed \"s/^[[:alnum:]]*[[:space:]]*\\([[:digit:]]*\\).*/\\1/\")\n",
    "```\n",
    "\n",
    "При этом в логе должно появиться\n",
    "```\n",
    "INFO[analyzer_state_manager.py:174] Received SIGINT 1/3. Will attempt to exit after current measurement.\n",
    "```\n",
    "\n",
    "Это означает, что model-analyzer дождётся окончания текущего измерения и после этого сохранит текущий статус в checkpoint, из которого можно уже сделать предварительный анализ (см. дальше)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!model-analyzer profile -f profile_config_manual.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**После завершения профилировки** необходимо запустить анализ. Он выполнится очень быстро, никакой нагрузки на GPU он не создаёт.\n",
    "Самые интересные результаты окажутся в папке results (сейчас там лежат пример результатов на NVIDIA A10). PDF-отчёт будет лежать в папке reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!model-analyzer analyze -f profile_config_manual.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# чтобы верифицировать yaml синтаксически\n",
    "import yaml\n",
    "with open(\"profile_config_manual.yaml\") as pc:\n",
    "    pc = yaml.safe_load(pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Navigator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Navigator не установлен в контейнере с Triton SDK. Его контейнер необходимо собрать самостоятельно из исходников. [Инструкция.](https://github.com/triton-inference-server/model_navigator/blob/main/docs/quick_start.md#install-the-triton-model-navigator-and-run-container)\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/triton-inference-server/model_navigator.git\n",
    "# Optional\n",
    "# git checkout v0.2.2\n",
    "make docker\n",
    "cd ..\n",
    "\n",
    "docker run -it --rm \\\n",
    " --gpus 1 \\\n",
    " -v /var/run/docker.sock:/var/run/docker.sock \\\n",
    " -v ${PWD}:${PWD} \\\n",
    " -w ${PWD} \\\n",
    " --net host \\\n",
    " --name model-navigator \\\n",
    " model-navigator /bin/bash\n",
    "```\n",
    "\n",
    "Конфигурация для Model Navigator хранится в файле [navigator_config.yaml](navigator_config.yaml). При работе Model Navigator склонен перезаписывать этот файл, поэтому команда запуска внутри контейнера предлагается такая:\n",
    "\n",
    "```\n",
    "cp navigator_config.yaml navigator_config_run.yaml; model-navigator run --config-path navigator_config_run.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
